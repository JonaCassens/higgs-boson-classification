{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ec5e8c",
   "metadata": {},
   "source": [
    "# Project Summary: Higgs → ττ Classification\n",
    "\n",
    "This notebook gives a simple written summary of the work done in the other notebooks:\n",
    "- Exploratory Data Analysis\n",
    "- Binary Classification (signal vs background)\n",
    "- Multiclass Classification (Z vs ggH vs VBF)\n",
    "- Main MRes assignment context\n",
    "\n",
    "Goal: Measure the Higgs signal strengths for gluon fusion (μ_ggH) and vector boson fusion (μ_VBF) with precision close to CMS reference values (≈9% and ≈18%). A machine learning approach using XGBoost creates smarter bins for a likelihood fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c1f5c",
   "metadata": {},
   "source": [
    "## Methods (Brief)\n",
    "\n",
    "1. Load data for three decay channels: et (electron–tau), mt (muon–tau), tt (tau–tau).\n",
    "2. Keep processes separate: Z (background), ggH and VBF (signal).\n",
    "3. Clean missing values (−9999 → median). No complex feature engineering required.\n",
    "4. Train:\n",
    "   - Binary models (merged channels, with channel indicator columns) for a simple baseline.\n",
    "   - Multiclass XGBoost models separately per channel for Z vs ggH vs VBF.\n",
    "5. Turn multiclass probabilities into a 2D grid of bins:\n",
    "   - x: P(ggH)/(P(ggH)+P(VBF)) distinguishes production mode.\n",
    "   - y: P(ggH)+P(VBF) distinguishes signal vs background.\n",
    "6. Scale each process histogram to expected yields (Z × 8.4, ggH × 0.034, VBF × 0.011).\n",
    "7. Combine channel histograms and fit Poisson likelihood to extract μ_ggH and μ_VBF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58067516",
   "metadata": {},
   "source": [
    "## Binary Classification (Baseline)\n",
    "\n",
    "Purpose: Quick check that ML can separate signal (ggH+VBF) from Z background.\n",
    "\n",
    "Highlights:\n",
    "- XGBoost gave the strongest accuracy/AUC among tested models (Random Forest, simple Neural Network).\n",
    "- Provides clean signal vs background ordering for 1D histograms.\n",
    "- Limitation: Cannot tell ggH from VBF, so not ideal for separate μ measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17294ca",
   "metadata": {},
   "source": [
    "## Multiclass Classification (Main Approach)\n",
    "\n",
    "We train one XGBoost model per channel (et, mt, tt) with three classes: Z, ggH, VBF.\n",
    "\n",
    "Why better:\n",
    "- Preserves differences between channels.\n",
    "- Separates ggH and VBF so we can measure μ_ggH and μ_VBF individually.\n",
    "\n",
    "Performance:\n",
    "- Good accuracy and confusion matrices show clear separation from Z.\n",
    "- Most confusion is between ggH and VBF (expected – similar final states)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc55ce8",
   "metadata": {},
   "source": [
    "## Probability Binning & Scaling\n",
    "\n",
    "We turn the multiclass probabilities into 2D bins:\n",
    "- Horizontal (production): P(ggH) / (P(ggH)+P(VBF))\n",
    "- Vertical (signal‑likeness): P(ggH)+P(VBF)\n",
    "\n",
    "Each process histogram is then scaled to expected yields (physics cross‑sections) so the likelihood fit reflects real rates rather than raw counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd394c",
   "metadata": {},
   "source": [
    "## Likelihood Fit (Simple View)\n",
    "\n",
    "We fit Poisson counts in all bins to extract:\n",
    "- μ_ggH (gluon fusion signal strength)\n",
    "- μ_VBF (vector boson fusion signal strength)\n",
    "- Also a single merged μ (optional)\n",
    "\n",
    "Precision = (uncertainty / value) × 100%. Targets: ggH ≤ 9%, VBF ≤ 18%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba610f",
   "metadata": {},
   "source": [
    "## Conclusion & Simple Next Steps\n",
    "\n",
    "The multiclass, per‑channel XGBoost approach provides clearer separation and enables measuring μ_ggH and μ_VBF with useful precision. The probability‑based 2D binning is a practical improvement over a single binary score.\n",
    "\n",
    "Next simple improvements:\n",
    "- Try a few more x/y bin combinations.\n",
    "- Tune XGBoost depth and learning rate.\n",
    "- Calibrate probabilities (e.g. isotonic) if needed.\n",
    "\n",
    "Overall, the notebooks show a complete path: clean data → train models → build smart bins → fit μ values. This matches the project goal in a straightforward way."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
